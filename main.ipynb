{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data: 10000 entries with server performance metrics\n",
    "n_samples = 10000\n",
    "\n",
    "# Generating random server performance metrics\n",
    "data = pd.DataFrame({\n",
    "    'cpu_usage': np.random.uniform(0, 100, n_samples),       # CPU usage in percentage\n",
    "    'memory_usage': np.random.uniform(0, 100, n_samples),    # Memory usage in percentage\n",
    "    'disk_io': np.random.uniform(0, 1000, n_samples),        # Disk I/O in MB/s\n",
    "    'network_io': np.random.uniform(0, 1000, n_samples),     # Network I/O in MB/s\n",
    "    'temperature': np.random.uniform(20, 100, n_samples),    # Server temperature in Celsius\n",
    "    'time_since_last_maintenance': np.random.uniform(0, 365, n_samples),  # Days since last maintenance\n",
    "})\n",
    "\n",
    "# Simulate failure labels: 1 = failure, 0 = no failure\n",
    "# Assume servers with high CPU, memory, and temperature are more likely to fail\n",
    "data['failure'] = (data['cpu_usage'] > 85) & (data['memory_usage'] > 80) & (data['temperature'] > 80)\n",
    "data['failure'] = data['failure'].astype(int)\n",
    "\n",
    "# Show a snapshot of the data\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = data.drop('failure', axis=1)\n",
    "y = data['failure']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Development â€“ Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test_scaled)\n",
    "y_prob = rf.predict_proba(X_test_scaled)[:, 1]  # Probability estimates for AUC\n",
    "\n",
    "# Evaluation: Confusion Matrix, Classification Report, and ROC AUC\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f'ROC AUC Score: {roc_auc_score(y_test, y_prob)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearch\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "# Best ROC AUC score\n",
    "print(f'Best ROC AUC: {grid_search.best_score_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Plot feature importance\n",
    "sns.barplot(x=importances, y=feature_names)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSmodules",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
